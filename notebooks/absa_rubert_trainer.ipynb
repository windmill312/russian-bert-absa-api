{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "absa-rubert-trainer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LkxWxUZAicQb",
        "colab": {}
      },
      "source": [
        "import xml.etree.cElementTree as ET\n",
        "import random\n",
        "import json\n",
        "import tokenization \n",
        "import torch\n",
        "import sys\n",
        "import torch.nn.functional as F\n",
        "from modeling import BertConfig, BertForSequenceMultiClassification\n",
        "from optimization import BERTAdam\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Va2UZL6s21_",
        "colab": {}
      },
      "source": [
        "with open(\"train.json\", \"r\") as read_file:\n",
        "    train = json.load(read_file)\n",
        "with open(\"test.json\", \"r\") as read_file:\n",
        "    test = json.load(read_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEWzuDcerLgs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "65746d27-1ab2-4b9d-b912-1316e9a2cc67"
      },
      "source": [
        "categories = []\n",
        "polarity = []\n",
        "for anno in train['annotation']:\n",
        "    for cat in anno['category']:\n",
        "        categories.append(cat)\n",
        "    for pol in anno['polarity']:\n",
        "        polarity.append(pol)\n",
        "num_classes = len(set(categories))\n",
        "print(num_classes)\n",
        "categories = list(set(categories))\n",
        "print(categories)\n",
        "num_pol = len(set(polarity))\n",
        "print(num_pol)\n",
        "polarity = list(set(polarity))\n",
        "print(polarity)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12\n",
            "['LOCATION#GENERAL', 'AMBIENCE#GENERAL', 'DRINKS#PRICES', 'RESTAURANT#PRICES', 'FOOD#STYLE_OPTIONS', 'RESTAURANT#MISCELLANEOUS', 'DRINKS#QUALITY', 'RESTAURANT#GENERAL', 'FOOD#QUALITY', 'DRINKS#STYLE_OPTIONS', 'FOOD#PRICES', 'SERVICE#GENERAL']\n",
            "5\n",
            "['negative', '', 'conflict', 'neutral', 'positive']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhWrbVihrLgw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "361655bf-274c-43ce-bc39-41c1fcb4174d"
      },
      "source": [
        "train['annotation'][1]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'category': ['AMBIENCE#GENERAL', 'AMBIENCE#GENERAL'],\n",
              " 'polarity': ['positive', 'positive'],\n",
              " 'text': 'Он был в уютном уголке в конце главного зала, приглушенный свет это основная часть этого ресторана там нет дневного освещения это было большим плюсом для нашего дня рожденья!'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ALdsLz7SiJEc",
        "colab": {}
      },
      "source": [
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, label):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.label = label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HlzlFI9SiJEg",
        "colab": {}
      },
      "source": [
        "tokenizer = tokenization.FullTokenizer(vocab_file='vocab.txt', do_lower_case=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DZKM06HGiJEj",
        "colab": {}
      },
      "source": [
        "def convert_examples_to_features(data, max_seq_length, tokenizer, num_classes, categories):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    features = []\n",
        "    for idx in range(len(data['annotation'])):\n",
        "        tokens_a = tokenizer.tokenize(data['annotation'][idx]['text'])\n",
        "        \n",
        "        # Account for [CLS] and [SEP] with \"- 2\"\n",
        "        if len(tokens_a) > max_seq_length - 2:\n",
        "            tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
        "            \n",
        "        tokens = []\n",
        "        tokens.append(\"[CLS]\")\n",
        "        for token in tokens_a:\n",
        "            tokens.append(token)\n",
        "        tokens.append(\"[SEP]\")\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        while len(input_ids) < max_seq_length:\n",
        "            input_ids.append(0)\n",
        "            input_mask.append(0)\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        \n",
        "        label = [0 for i in range(num_classes)]\n",
        "        for i in range(len(data['annotation'][idx]['category'])):\n",
        "            ind = categories.index(data['annotation'][idx]['category'][i])\n",
        "            inde = polarity.index(data['annotation'][idx]['polarity'][i])\n",
        "            label[ind] = inde + 1\n",
        "            \n",
        "        features.append(\n",
        "                InputFeatures(\n",
        "                        input_ids=input_ids,\n",
        "                        input_mask=input_mask,\n",
        "                        label=label))\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6rDdRo9giJEn",
        "colab": {}
      },
      "source": [
        "class Dataload(torch.utils.data.Dataset):\n",
        "    def __init__(self, features):\n",
        "        self.features = features\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return torch.LongTensor(self.features[index].input_ids),\\\n",
        "               torch.LongTensor(self.features[index].input_mask),\\\n",
        "               torch.LongTensor(self.features[index].label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3P2xiA00iJEq",
        "colab": {}
      },
      "source": [
        "features = convert_examples_to_features(train, 512, tokenizer, num_classes, categories)\n",
        "dataset_train = Dataload(features)\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset_train,batch_size = 8, shuffle=True,\\\n",
        "                                               num_workers=6, pin_memory=True)\n",
        "features = convert_examples_to_features(test, 512, tokenizer, num_classes, categories)\n",
        "dataset_test = Dataload(features)\n",
        "test_dataloader = torch.utils.data.DataLoader(dataset_test,batch_size = 1, shuffle=False,\\\n",
        "                                               num_workers=6, pin_memory=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBmWdA-6rLhE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "20d630be-253f-4744-feca-04bfada7ceef"
      },
      "source": [
        "next(iter(train_dataloader))[2][0]"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZTas2ChtsnS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7fc62809-680b-4e45-eccf-6dac5abd0024"
      },
      "source": [
        "#!wget http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_pt.tar.gz\n",
        "#!tar -xf rubert_cased_L-12_H-768_A-12_pt.tar.gz\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qCAZ7sCoiJEs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "d36e9da6-4380-404c-bdd4-6a34167f4012"
      },
      "source": [
        "device = 'cuda'\n",
        "bert_config = BertConfig.from_json_file('bert_config.json')\n",
        "model = BertForSequenceMultiClassification(bert_config, num_classes)\n",
        "model.bert.load_state_dict(torch.load('/content/drive/My Drive/pytorch_model.bin', map_location='cpu'))\n",
        "model.to(device)\n",
        "model = torch.nn.DataParallel(model)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-47d6a9231bbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceMultiClassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/pytorch_model.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-PQpSuHliJEu",
        "colab": {}
      },
      "source": [
        "num_epoch = 10\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_parameters = [\n",
        "     {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
        "     {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
        "     ]\n",
        "num_train_steps = len(test_dataloader) * num_epoch\n",
        "optimizer = BERTAdam(optimizer_parameters,\n",
        "                     lr=5e-5,\n",
        "                     warmup=0.1,\n",
        "                     t_total=num_train_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OdHdxpvAiJEw",
        "colab": {}
      },
      "source": [
        "global_step = 0\n",
        "for _ in range(num_epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_step = 0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, input_mask, label = batch\n",
        "        loss, logits = model(input_ids, None, input_mask, label)\n",
        "        loss = loss.mean()\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        log = '\\r {ep:d}/{len_d:d} {loss:.4f}\\r'.format(ep = step, len_d=len(train_dataloader), loss=loss.item())\n",
        "        sys.stdout.write(log)\n",
        "        sys.stdout.flush()\n",
        "#         if step%10 == 0:\n",
        "#             print(loss.item(), loss_pol.item())\n",
        "        if (step + 1) % 1 == 0:\n",
        "            optimizer.step()    # We have accumulated enought gradients\n",
        "            model.zero_grad()\n",
        "            global_step += 1\n",
        "        total_step += 1\n",
        "    print(total_loss/total_step)\n",
        "    total_loss = 0    \n",
        "    total_step = 0\n",
        "    acc = [0 for _ in range(num_classes)]  \n",
        "    model.eval()\n",
        "    for step, batch in enumerate(test_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, input_mask, label = batch\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            logits = model(input_ids, None, input_mask)\n",
        "        for i, ex in enumerate(logits):\n",
        "            ex = F.softmax(ex, dim=-1)\n",
        "            idx = ex.argmax()\n",
        "            if idx == label[0][i].item():\n",
        "                acc[i] += 1\n",
        "        total_step += 1\n",
        "    mean = []\n",
        "    for i in range(num_classes):\n",
        "        mean.append(acc[i]/total_step)\n",
        "        print(categories[i],acc[i]/total_step)\n",
        "    print(sum(mean)/num_classes)\n",
        "    print('================')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUOXfzBjDRr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the model to disk\n",
        "filename = 'new_final_model.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rxh84oKXurcF",
        "colab": {}
      },
      "source": [
        "def predict(text, filename):\n",
        "  final_model = pickle.load(open(filename, 'rb'))\n",
        "  final_model.eval()\n",
        "  tokens_a = tokenizer.tokenize(text)\n",
        "  if len(tokens_a) > 512 - 2:\n",
        "      tokens_a = tokens_a[0:(512 - 2)]\n",
        "              \n",
        "  tokens = []\n",
        "  tokens.append(\"[CLS]\")\n",
        "  for token in tokens_a:\n",
        "      tokens.append(token)\n",
        "  tokens.append(\"[SEP]\")\n",
        "\n",
        "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "  input_mask = [1] * len(input_ids)\n",
        "  while len(input_ids) < 512:\n",
        "      input_ids.append(0)\n",
        "      input_mask.append(0)\n",
        "  input_ids = torch.LongTensor(input_ids).unsqueeze(0)\n",
        "  input_mask = torch.LongTensor(input_mask).unsqueeze(0)\n",
        "  logits = final_model(input_ids, None, input_mask)\n",
        "  for i, ex in enumerate(logits):\n",
        "      ex = F.softmax(ex, dim=-1)\n",
        "      idx = ex.argmax()\n",
        "      if idx>0:\n",
        "          print(categories[i], polarity[idx-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tMCu2770iJEy",
        "colab": {}
      },
      "source": [
        "filename = 'new_final_model.sav'\n",
        "predict('Еда, конечно, хороша, но вот обслуживание оставляет желать лучшего', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6RvOUF78vNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('Обслуживание было выще всяческих похвал', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3J8znkZ9EQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('Атмосфера в ресторане была удручающей, пока нам не принесли оригинальные коктейли, вкус которых позволил нам насладиться богатым разнообразием кухни сполна', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkVHafzH9y4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('В следующий раз, когда захочу испортить себе вечер, обязательно приду в этот ресторан', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSg7aTM4-Ehc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('Если хотите испортить себе настроение, приходите в этот ресторан', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfIYjZH6-MUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('Соли в суп могли бы и поменьше добавить', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gzbwr0zX-WHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('По вкусу такое ощущение как-будто повар в первый раз готовил это блюдо', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVA_GmhM-jPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('Чек вышел средний по Москве', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldAX0eCH-9YP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('Отдали дофига бабла', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z03Xz62a_Box",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('Несмотря на изысканное оформление блюд, цены нас очень даже порадовали', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FidugZby_n-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('Вид на Москву-реку испортил весь аппетит', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnrdBSai_wwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('Все было отлично, пока официанта вырвало прямо на меня', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcl_bwEx_79I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('Отдал 5000 р за два блюда, кошмар', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5J-VMDYAKYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('Еда дешевая, напитки дорогие', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to-QcVSFAT-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('Все классно, но музыку могли бы и потише сделать', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgMBzLGPAnVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('Больше не приду', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR_fkYKxAtMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('У официантки не было глаза', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AlJhS91BLo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('Мужчина нашел волос в хлебе', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVana216BZqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('Из-за короновируса заведение было закрыто', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmuEdL38Bh8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('Отказали на мою просьбу позвать администратора', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfXl5U8uByEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('Ром кола стоит копейки', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmvXQcQsB-Sn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('Барашек просто пэрсик', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xPWOjiHCIIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('Официантка Зоя просто вах', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2bIEUJPCSuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('Стейк был недожарен', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWkpaDd9Eo9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('Стейк был дожарен', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}